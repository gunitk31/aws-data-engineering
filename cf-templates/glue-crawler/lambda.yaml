AWSTemplateFormatVersion: '2010-09-09'

Parameters:
  ParamCrawlerS3BucketName:
    Type: String
    Description: "Name of the S3 bucket"

Resources:

  svcCrawlerLambdaExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      RoleName: !Sub svcCrawlerLambdaExecutionRole-${AWS::Region}
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: "lambda.amazonaws.com"
            Action: "sts:AssumeRole"
      Policies:
        - PolicyName: "StartGlueCrawlerLambdaPolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "glue:StartCrawler"
                  - "glue:GetCrawler"
                Resource: !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:crawler/${ParamCrawlerS3BucketName}-crawler"
              - Effect: Allow
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: "*"

  CrawlerTriggerLambda:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: !Sub "${ParamCrawlerS3BucketName}-crawler-trigger"
      Runtime: "python3.9"
      Handler: "index.lambda_handler"
      Role: !GetAtt svcCrawlerLambdaExecutionRole.Arn
      Timeout: 900
      Environment:
        Variables:
          CrawlerName: !Sub "${ParamCrawlerS3BucketName}-crawler"
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import logging

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3_client = boto3.client('s3')
          glue_client = boto3.client('glue')

          def lambda_handler(event, context):
              crawler_name = os.environ.get("CrawlerName")

              if not crawler_name:
                  logger.error("Crawler name is missing")
                  return {"statusCode": 400, "body": json.dumps({"error": "Crawler name is missing"})}

              try:
                  for record in event.get("Records", []):
                      bucket_name = record["s3"]["bucket"]["name"]
                      object_key = record["s3"]["object"]["key"]

                      # Detect if the object is a top-level folder (ends with /)
                      if if object_key.count("/") == 1 and object_key.endswith("/"):
                          logger.info(f"New top-level folder created: {object_key} in bucket {bucket_name}")
                          continue  # No need to start the crawler for empty folders

                      # Detect if it's the first file in the folder
                      folder_path = "/".join(object_key.split("/")[:-1]) + "/"
                      response = s3_client.list_objects_v2(
                          Bucket=bucket_name,
                          Prefix=folder_path
                      )
                      
                      num_files = sum(1 for obj in response.get("Contents", []) if not obj["Key"].endswith("/"))

                      if num_files > 1:
                          logger.info(f"File uploaded in existing folder: {object_key}, skipping crawler start")
                          continue

                      logger.info(f"First file uploaded in folder: {folder_path}, triggering Glue Crawler")

                      # Check if the crawler is already running
                      crawler_info = glue_client.get_crawler(Name=crawler_name)
                      crawler_state = crawler_info["Crawler"]["State"]

                      if crawler_state == "RUNNING":
                          logger.info(f"Crawler {crawler_name} is already running")
                          return {"statusCode": 200, "body": json.dumps({"message": f"Crawler {crawler_name} is already running"})}

                      # Start the crawler if it's not running
                      glue_client.start_crawler(Name=crawler_name)
                      logger.info(f"Crawler {crawler_name} started")

                      return {"statusCode": 200, "body": json.dumps({"message": f"Crawler {crawler_name} started"})}

              except Exception as e:
                  logger.error(f"Error processing event: {str(e)}", exc_info=True)
                  return {
                      "statusCode": 500,
                      "body": json.dumps({"error": str(e)})
                  }

Outputs:
  CrawlerTriggerLambdaArn:
    Description: "ARN of the Lambda function that triggers the Glue Crawler"
    Value: !GetAtt CrawlerTriggerLambda.Arn
    Export:
      Name: CrawlerTriggerLambdaArn